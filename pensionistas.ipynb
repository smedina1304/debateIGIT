{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Início\n",
    "\n",
    "#### Importação das Libs e criação da sessão spark"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .getOrCreate()\n",
    ")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/09/17 18:55:37 WARN Utils: Your hostname, iMac-de-Sergio-2.local resolves to a loopback address: 127.0.0.1; using 192.168.0.222 instead (on interface en0)\n",
      "21/09/17 18:55:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "21/09/17 18:55:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "# Local Datalake Paths\n",
    "local_zones = {\n",
    "    'landing': './data/landing_zone',\n",
    "    'raw': './data/raw_zone',\n",
    "    'staging': './data/staging_zone',\n",
    "    'consumer': './data/consumer_zone',\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Etapa 1 - Download do Dataset de Pensionistas"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Link para download do arquivo de pensionistas\n",
    "zipUrl = 'http://repositorio.dados.gov.br/segrt/pensionistas/PENSIONISTAS_082021.zip'\n",
    "\n",
    "### Landing Zone\n",
    "# Path do diretorio para gravação do arquivo\n",
    "outPath = f'{local_zones[\"landing\"]}/pensionistas'\n",
    "os.makedirs(outPath, exist_ok=True)\n",
    "    \n",
    "# baixar arquivo direto da fonte\n",
    "req = requests.get(zipUrl)\n",
    "zipOut = f'{outPath}/PENSIONISTAS.zip'\n",
    "print('Saving:', zipOut)\n",
    "with open(zipOut, 'wb') as f:\n",
    "    f.write(req.content)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "### Raw Zone\n",
    "outPath = f'{local_zones[\"raw\"]}/pensionistas'\n",
    "os.makedirs(outPath, exist_ok=True)\n",
    "\n",
    "# Unzip - Extract all\n",
    "with zipfile.ZipFile(zipOut, 'r') as zipObj:\n",
    "   # Extract all\n",
    "   listOfFileNames = zipObj.namelist()\n",
    "   # varre a lista de arquivos - file names\n",
    "   for fileName in listOfFileNames:\n",
    "       # Apenas .csv\n",
    "       if fileName.endswith('.csv'):\n",
    "           print('Unzip:', fileName, ' -> ', outPath)\n",
    "           zipObj.extract(fileName, path=outPath)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saving: ./data/landing_zone/pensionistas/PENSIONISTAS.zip\n",
      "Unzip: PENSIONISTAS_082021.csv  ->  ./data/raw_zone/pensionistas\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Etapa 02 - Leitura do dataframe da Raw e formatação para Staging"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "csvFile = f'{local_zones[\"raw\"]}/pensionistas/{fileName}'\n",
    "\n",
    "# Carregando o Dataframe\n",
    "df_origem = (\n",
    "    spark.read\n",
    "    .format('csv')\n",
    "    .option('sep',';')\n",
    "    .option('header',True)\n",
    "    .load(csvFile)\n",
    ")\n",
    "\n",
    "# Selecionando as Colunas representativas\n",
    "cols = ['NOME DO BENEFICIARIO', \n",
    "        'NOME DO ORGAO',\n",
    "        'DATA DE NASCIMENTO',\n",
    "        'UF DA UPAG DE VINCULACAO',\n",
    "        'NATUREZA PENSAO',\n",
    "        'DATA INICIO DO BENEFICIO',\n",
    "        'DATA FIM DO BENEFICIO',\n",
    "        'RENDIMENTO LIQUIDO',\n",
    "        'PAGAMENTO SUSPENSO']\n",
    "\n",
    "# Renomeando das colunas\n",
    "df_pensionistas = (\n",
    "    df_origem\n",
    "    .select(cols)\n",
    "    .withColumnRenamed('NOME DO BENEFICIARIO','nome')\n",
    "    .withColumnRenamed('NOME DO ORGAO','orgao')\n",
    "    .withColumnRenamed('DATA DE NASCIMENTO','dtnasc')\n",
    "    .withColumnRenamed('UF DA UPAG DE VINCULACAO','uf')\n",
    "    .withColumnRenamed('NATUREZA PENSAO','natpensao')\n",
    "    .withColumnRenamed('DATA INICIO DO BENEFICIO','dtiniben')\n",
    "    .withColumnRenamed('DATA FIM DO BENEFICIO','dtfimben')\n",
    "    .withColumnRenamed('RENDIMENTO LIQUIDO','rendLiquido')\n",
    "    .withColumnRenamed('PAGAMENTO SUSPENSO','pagsuspenso')\n",
    ")\n",
    "\n",
    "\n",
    "# Trantando e convertendo as colunas\n",
    "df_pensionistas = (\n",
    "    df_pensionistas\n",
    "    .select('nome', 'orgao', 'dtnasc', 'uf', 'natpensao', 'dtiniben', 'dtfimben', 'rendLiquido', 'pagsuspenso')\n",
    "    .withColumn('dtnasc', f.to_date( f.col('dtnasc'),'ddMMyyyy') )\n",
    "    .withColumn('dtiniben', f.to_date( f.col('dtiniben'),'ddMMyyyy') )\n",
    "    .withColumn('dtfimben', f.to_date( f.col('dtfimben'),'ddMMyyyy') )\n",
    "    .withColumn('rendLiquido', f.regexp_replace(f.regexp_replace(f.col('rendLiquido'), \"\\\\.\", \"\"), \"\\\\,\", \".\"))\n",
    "    .withColumn('rendLiquido', f.col('rendLiquido').cast('float') )\n",
    "    .withColumn('pagsuspenso', f.when(f.col('pagsuspenso') == 'NAO', False)\n",
    "                            .when(f.col('pagsuspenso') == 'SIM', True)\n",
    "                            .otherwise(None) )\n",
    "    .withColumn('limitMax35', f.expr('round(rendLiquido*0.35,2)') )\n",
    "    .withColumn('limitMax40', f.expr('round(rendLiquido*0.40,2)') )\n",
    ")\n",
    "\n",
    "\n",
    "del df_origem"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_pensionistas.limit(5).toPandas()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "(\n",
    "    df_pensionistas\n",
    "    .groupBy('pagsuspenso')\n",
    "    .agg(f.count('nome'))\n",
    "    .toPandas()\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "(\n",
    "    df_pensionistas\n",
    "    .groupBy('uf','natpensao','pagsuspenso')\n",
    "    .agg(f.count('nome'))\n",
    "    .toPandas()\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "### Gravação na Staging\n",
    "outPath = f'{local_zones[\"staging\"]}/pensionistas'\n",
    "df_pensionistas.write.format('parquet').save(outPath, mode='overwrite')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "interpreter": {
   "hash": "96b577b15a68a61d495e34938adaa9df3d6beac09e53842e3114afcb7393acc8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}