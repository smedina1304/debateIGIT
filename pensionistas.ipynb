{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Início\n",
    "\n",
    "#### Importação das Libs e criação da sessão spark"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from pyspark.sql import SparkSession\r\n",
    "from pyspark.sql.types import *\r\n",
    "\r\n",
    "import pyspark.sql.functions as f\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "import requests\r\n",
    "import zipfile\r\n",
    "import os\r\n",
    "\r\n",
    "import findspark\r\n",
    "findspark.init()\r\n",
    "\r\n",
    "spark = (\r\n",
    "    SparkSession\r\n",
    "    .builder\r\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\r\n",
    "    .getOrCreate()\r\n",
    ")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pyspark.sql.functions as f\r\n",
    "\r\n",
    "# Local Datalake Paths\r\n",
    "local_zones = {\r\n",
    "    'landing': './data/landing_zone',\r\n",
    "    'raw': './data/raw_zone',\r\n",
    "    'staging': './data/staging_zone',\r\n",
    "    'consumer': './data/consumer_zone',\r\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Etapa 1 - Download do Dataset de Pensionistas"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Link para download do arquivo de pensionistas\r\n",
    "zipUrl = 'http://repositorio.dados.gov.br/segrt/pensionistas/PENSIONISTAS_082021.zip'\r\n",
    "\r\n",
    "### Landing Zone\r\n",
    "# Path do diretorio para gravação do arquivo\r\n",
    "outPath = f'{local_zones[\"landing\"]}/pensionistas'\r\n",
    "os.makedirs(outPath, exist_ok=True)\r\n",
    "    \r\n",
    "# baixar arquivo direto da fonte\r\n",
    "req = requests.get(zipUrl)\r\n",
    "zipOut = f'{outPath}/PENSIONISTAS.zip'\r\n",
    "print('Saving:', zipOut)\r\n",
    "with open(zipOut, 'wb') as f:\r\n",
    "    f.write(req.content)\r\n",
    "    f.close()\r\n",
    "\r\n",
    "\r\n",
    "### Raw Zone\r\n",
    "outPath = f'{local_zones[\"raw\"]}/pensionistas'\r\n",
    "os.makedirs(outPath, exist_ok=True)\r\n",
    "\r\n",
    "# Unzip - Extract all\r\n",
    "with zipfile.ZipFile(zipOut, 'r') as zipObj:\r\n",
    "   # Extract all\r\n",
    "   listOfFileNames = zipObj.namelist()\r\n",
    "   # varre a lista de arquivos - file names\r\n",
    "   for fileName in listOfFileNames:\r\n",
    "       # Apenas .csv\r\n",
    "       if fileName.endswith('.csv'):\r\n",
    "           print('Unzip:', fileName, ' -> ', outPath)\r\n",
    "           zipObj.extract(fileName, path=outPath)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saving: ./data/landing_zone/pensionistas/PENSIONISTAS.zip\n",
      "Unzip: PENSIONISTAS_082021.csv  ->  ./data/raw_zone/pensionistas\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Etapa 02 - Leitura do dataframe da Raw e formatação para Staging"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "import pyspark.sql.functions as f\r\n",
    "\r\n",
    "csvFile = f'{local_zones[\"raw\"]}/pensionistas/{fileName}'\r\n",
    "\r\n",
    "# Carregando o Dataframe\r\n",
    "df_origem = (\r\n",
    "    spark.read\r\n",
    "    .format('csv')\r\n",
    "    .option('sep',';')\r\n",
    "    .option('header',True)\r\n",
    "    .load(csvFile)\r\n",
    ")\r\n",
    "\r\n",
    "# Selecionando as Colunas representativas\r\n",
    "cols = ['NOME DO BENEFICIARIO', \r\n",
    "        'NOME DO ORGAO',\r\n",
    "        'DATA DE NASCIMENTO',\r\n",
    "        'UF DA UPAG DE VINCULACAO',\r\n",
    "        'NATUREZA PENSAO',\r\n",
    "        'DATA INICIO DO BENEFICIO',\r\n",
    "        'DATA FIM DO BENEFICIO',\r\n",
    "        'RENDIMENTO LIQUIDO',\r\n",
    "        'PAGAMENTO SUSPENSO']\r\n",
    "\r\n",
    "# Renomeando das colunas\r\n",
    "df_pensionistas = (\r\n",
    "    df_origem\r\n",
    "    .select(cols)\r\n",
    "    .withColumnRenamed('NOME DO BENEFICIARIO','nome')\r\n",
    "    .withColumnRenamed('NOME DO ORGAO','orgao')\r\n",
    "    .withColumnRenamed('DATA DE NASCIMENTO','dtnasc')\r\n",
    "    .withColumnRenamed('UF DA UPAG DE VINCULACAO','uf')\r\n",
    "    .withColumnRenamed('NATUREZA PENSAO','natpensao')\r\n",
    "    .withColumnRenamed('DATA INICIO DO BENEFICIO','dtiniben')\r\n",
    "    .withColumnRenamed('DATA FIM DO BENEFICIO','dtfimben')\r\n",
    "    .withColumnRenamed('RENDIMENTO LIQUIDO','rendLiquido')\r\n",
    "    .withColumnRenamed('PAGAMENTO SUSPENSO','pagsuspenso')\r\n",
    ")\r\n",
    "\r\n",
    "\r\n",
    "# Trantando e convertendo as colunas\r\n",
    "df_pensionistas = (\r\n",
    "    df_pensionistas\r\n",
    "    .select('nome', 'orgao', 'dtnasc', 'uf', 'natpensao', 'dtiniben', 'dtfimben', 'rendLiquido', 'pagsuspenso')\r\n",
    "    .withColumn('dtnasc', f.to_date( f.col('dtnasc'),'ddMMyyyy') )\r\n",
    "    .withColumn('dtiniben', f.to_date( f.col('dtiniben'),'ddMMyyyy') )\r\n",
    "    .withColumn('dtfimben', f.to_date( f.col('dtfimben'),'ddMMyyyy') )\r\n",
    "    .withColumn('rendLiquido', f.regexp_replace(f.regexp_replace(f.col('rendLiquido'), \"\\\\.\", \"\"), \"\\\\,\", \".\"))\r\n",
    "    .withColumn('rendLiquido', f.col('rendLiquido').cast('float') )\r\n",
    "    .withColumn('pagsuspenso', f.when(f.col('pagsuspenso') == 'NAO', False)\r\n",
    "                            .when(f.col('pagsuspenso') == 'SIM', True)\r\n",
    "                            .otherwise(None) )\r\n",
    "    .withColumn('limitMax35', f.expr('round(rendLiquido*0.35,2)').cast('float') )\r\n",
    "    .withColumn('limitMax40', f.expr('round(rendLiquido*0.40,2)').cast('float') )\r\n",
    "    .withColumn('faixaRenda', f.when(f.col('rendLiquido').between(0,3000),'Entre 0 e 3000')\r\n",
    "                             .when(f.col('rendLiquido').between(3001,7000),'Entre 3001 e 7000')\r\n",
    "                             .when(f.col('rendLiquido').between(7001,15000),'Entre 7001 e 15000')\r\n",
    "                             .when(f.col('rendLiquido').between(15001,30000),'Entre 15001 e 30000')\r\n",
    "                             .otherwise('Acima de 30.000') )\r\n",
    "    .withColumn('idade' , f.floor( f.datediff( f.current_date(), f.col('dtnasc') ) / 365))\r\n",
    "    .withColumn('faixaIdade', f.when(f.col('idade').between(0,15),'Entre 0 e 15 anos')\r\n",
    "                             .when(f.col('idade').between(16,30),'Entre 16 e 30 anos')\r\n",
    "                             .when(f.col('idade').between(31,60),'Entre 31 e 60 anos')\r\n",
    "                             .otherwise('Acima de 60 anos') )\r\n",
    ")\r\n",
    "\r\n",
    "del df_origem"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "df_pensionistas.limit(5).toPandas()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                              nome                                    orgao  \\\n",
       "0         MARIA DA CRUZ DOS SANTOS  MINIST.DA AGRICULTURA,PECUARIA E ABAST.   \n",
       "1  CARMEN LUCINDA FARKAS DE ARAUJO  MINIST.DA AGRICULTURA,PECUARIA E ABAST.   \n",
       "2        MARIA DE LOURDES DA SILVA  MINIST.DA AGRICULTURA,PECUARIA E ABAST.   \n",
       "3       DIOMARINA ALVES DOS SANTOS  MINIST.DA AGRICULTURA,PECUARIA E ABAST.   \n",
       "4              MARIZETE DIAS SOUZA  MINIST.DA AGRICULTURA,PECUARIA E ABAST.   \n",
       "\n",
       "       dtnasc  uf   natpensao    dtiniben dtfimben   rendLiquido  pagsuspenso  \\\n",
       "0  1955-01-28  DF   VITALICIA  2021-01-29     None   3039.580078        False   \n",
       "1  1951-04-18  DF   VITALICIA  1998-02-10     None   1216.250000        False   \n",
       "2  1948-01-14  DF  TEMPORARIA  2016-01-25     None   2697.459961        False   \n",
       "3  1944-06-18  DF   VITALICIA  2010-12-19     None   6953.770020        False   \n",
       "4  1954-03-18  DF   VITALICIA  2012-10-13     None  14994.889648        False   \n",
       "\n",
       "    limitMax35   limitMax40          faixaRenda  idade        faixaIdade  \n",
       "0  1063.849976  1215.829956   Entre 3001 e 7000     66  Acima de 60 anos  \n",
       "1   425.690002   486.500000      Entre 0 e 3000     70  Acima de 60 anos  \n",
       "2   944.109985  1078.979980      Entre 0 e 3000     73  Acima de 60 anos  \n",
       "3  2433.820068  2781.510010   Entre 3001 e 7000     77  Acima de 60 anos  \n",
       "4  5248.209961  5997.959961  Entre 7001 e 15000     67  Acima de 60 anos  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nome</th>\n",
       "      <th>orgao</th>\n",
       "      <th>dtnasc</th>\n",
       "      <th>uf</th>\n",
       "      <th>natpensao</th>\n",
       "      <th>dtiniben</th>\n",
       "      <th>dtfimben</th>\n",
       "      <th>rendLiquido</th>\n",
       "      <th>pagsuspenso</th>\n",
       "      <th>limitMax35</th>\n",
       "      <th>limitMax40</th>\n",
       "      <th>faixaRenda</th>\n",
       "      <th>idade</th>\n",
       "      <th>faixaIdade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MARIA DA CRUZ DOS SANTOS</td>\n",
       "      <td>MINIST.DA AGRICULTURA,PECUARIA E ABAST.</td>\n",
       "      <td>1955-01-28</td>\n",
       "      <td>DF</td>\n",
       "      <td>VITALICIA</td>\n",
       "      <td>2021-01-29</td>\n",
       "      <td>None</td>\n",
       "      <td>3039.580078</td>\n",
       "      <td>False</td>\n",
       "      <td>1063.849976</td>\n",
       "      <td>1215.829956</td>\n",
       "      <td>Entre 3001 e 7000</td>\n",
       "      <td>66</td>\n",
       "      <td>Acima de 60 anos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CARMEN LUCINDA FARKAS DE ARAUJO</td>\n",
       "      <td>MINIST.DA AGRICULTURA,PECUARIA E ABAST.</td>\n",
       "      <td>1951-04-18</td>\n",
       "      <td>DF</td>\n",
       "      <td>VITALICIA</td>\n",
       "      <td>1998-02-10</td>\n",
       "      <td>None</td>\n",
       "      <td>1216.250000</td>\n",
       "      <td>False</td>\n",
       "      <td>425.690002</td>\n",
       "      <td>486.500000</td>\n",
       "      <td>Entre 0 e 3000</td>\n",
       "      <td>70</td>\n",
       "      <td>Acima de 60 anos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MARIA DE LOURDES DA SILVA</td>\n",
       "      <td>MINIST.DA AGRICULTURA,PECUARIA E ABAST.</td>\n",
       "      <td>1948-01-14</td>\n",
       "      <td>DF</td>\n",
       "      <td>TEMPORARIA</td>\n",
       "      <td>2016-01-25</td>\n",
       "      <td>None</td>\n",
       "      <td>2697.459961</td>\n",
       "      <td>False</td>\n",
       "      <td>944.109985</td>\n",
       "      <td>1078.979980</td>\n",
       "      <td>Entre 0 e 3000</td>\n",
       "      <td>73</td>\n",
       "      <td>Acima de 60 anos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DIOMARINA ALVES DOS SANTOS</td>\n",
       "      <td>MINIST.DA AGRICULTURA,PECUARIA E ABAST.</td>\n",
       "      <td>1944-06-18</td>\n",
       "      <td>DF</td>\n",
       "      <td>VITALICIA</td>\n",
       "      <td>2010-12-19</td>\n",
       "      <td>None</td>\n",
       "      <td>6953.770020</td>\n",
       "      <td>False</td>\n",
       "      <td>2433.820068</td>\n",
       "      <td>2781.510010</td>\n",
       "      <td>Entre 3001 e 7000</td>\n",
       "      <td>77</td>\n",
       "      <td>Acima de 60 anos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MARIZETE DIAS SOUZA</td>\n",
       "      <td>MINIST.DA AGRICULTURA,PECUARIA E ABAST.</td>\n",
       "      <td>1954-03-18</td>\n",
       "      <td>DF</td>\n",
       "      <td>VITALICIA</td>\n",
       "      <td>2012-10-13</td>\n",
       "      <td>None</td>\n",
       "      <td>14994.889648</td>\n",
       "      <td>False</td>\n",
       "      <td>5248.209961</td>\n",
       "      <td>5997.959961</td>\n",
       "      <td>Entre 7001 e 15000</td>\n",
       "      <td>67</td>\n",
       "      <td>Acima de 60 anos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "(\r\n",
    "    df_pensionistas\r\n",
    "    .groupBy('pagsuspenso')\r\n",
    "    .agg(f.count('nome'))\r\n",
    "    .toPandas()\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   pagsuspenso  count(nome)\n",
       "0         True         2791\n",
       "1        False       289255"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pagsuspenso</th>\n",
       "      <th>count(nome)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>2791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>289255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "(\r\n",
    "    df_pensionistas\r\n",
    "    .groupBy('uf','natpensao','pagsuspenso')\r\n",
    "    .agg(f.count('nome'))\r\n",
    "    .toPandas()\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     uf   natpensao  pagsuspenso  count(nome)\n",
       "0    PA  TEMPORARIA         True           30\n",
       "1    PE  TEMPORARIA         True           31\n",
       "2    SE  TEMPORARIA        False          564\n",
       "3    PR  TEMPORARIA         True            3\n",
       "4    MA   VITALICIA         True           20\n",
       "..   ..         ...          ...          ...\n",
       "103  AL   VITALICIA        False         1663\n",
       "104  ES  TEMPORARIA         True            2\n",
       "105  AC  TEMPORARIA         True            1\n",
       "106  PA  TEMPORARIA        False         2255\n",
       "107  PE   VITALICIA         True           51\n",
       "\n",
       "[108 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uf</th>\n",
       "      <th>natpensao</th>\n",
       "      <th>pagsuspenso</th>\n",
       "      <th>count(nome)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PA</td>\n",
       "      <td>TEMPORARIA</td>\n",
       "      <td>True</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PE</td>\n",
       "      <td>TEMPORARIA</td>\n",
       "      <td>True</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SE</td>\n",
       "      <td>TEMPORARIA</td>\n",
       "      <td>False</td>\n",
       "      <td>564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PR</td>\n",
       "      <td>TEMPORARIA</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MA</td>\n",
       "      <td>VITALICIA</td>\n",
       "      <td>True</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>AL</td>\n",
       "      <td>VITALICIA</td>\n",
       "      <td>False</td>\n",
       "      <td>1663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>ES</td>\n",
       "      <td>TEMPORARIA</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>AC</td>\n",
       "      <td>TEMPORARIA</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>PA</td>\n",
       "      <td>TEMPORARIA</td>\n",
       "      <td>False</td>\n",
       "      <td>2255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>PE</td>\n",
       "      <td>VITALICIA</td>\n",
       "      <td>True</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "source": [
    "(\r\n",
    "    df_pensionistas\r\n",
    "    .groupBy('faixaRenda')\r\n",
    "    .agg(f.sum(f.col('rendliquido')).cast('float').alias('valor'))\r\n",
    "    .toPandas()\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            faixaRenda        valor\n",
       "0   Entre 7001 e 15000  281589312.0\n",
       "1  Entre 15001 e 30000  160190880.0\n",
       "2      Acima de 30.000    8150895.5\n",
       "3       Entre 0 e 3000  232925504.0\n",
       "4    Entre 3001 e 7000  550267136.0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>faixaRenda</th>\n",
       "      <th>valor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Entre 7001 e 15000</td>\n",
       "      <td>281589312.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Entre 15001 e 30000</td>\n",
       "      <td>160190880.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Acima de 30.000</td>\n",
       "      <td>8150895.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Entre 0 e 3000</td>\n",
       "      <td>232925504.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Entre 3001 e 7000</td>\n",
       "      <td>550267136.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 106
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "### Gravação na Staging\r\n",
    "outPath = f'{local_zones[\"staging\"]}/pensionistas'\r\n",
    "df_pensionistas.write.format('parquet').save(outPath, mode='overwrite')"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2348.save.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:645)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1230)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1435)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:493)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1868)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1910)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1868)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1910)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:332)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:402)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:375)\r\n\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:48)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:220)\r\n\t... 32 more\r\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-c7a805b9fc66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m### Gravação na Staging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0moutPath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'{local_zones[\"staging\"]}/pensionistas'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf_pensionistas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'parquet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutPath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'overwrite'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1107\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1109\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o2348.save.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:645)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1230)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1435)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:493)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1868)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1910)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1868)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1910)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:332)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:402)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:375)\r\n\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:48)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:220)\r\n\t... 32 more\r\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}